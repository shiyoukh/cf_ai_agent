# ğŸ§  AI Chat Agent (Cloudflare + Llama 3.3)

An intelligent real-time chatbot powered by **Llama 3.3 (70B Instruct)** on **Cloudflare Workers AI**.
This project runs a streaming chat agent with task scheduling and tool-integration support, fully deployable on Cloudflare.

---

## ğŸš€ Features

- âš¡ Powered by Llama 3.3 (70B-Instruct)
- ğŸ’¬ Real-time streaming chat responses
- ğŸ§© Extensible tool system for task automation
- ğŸ•’ Built-in schedule tool support
- â˜ï¸ Deployable on Cloudflare Workers
- ğŸ§± TypeScript-based, modular structure

---

## ğŸ§© Prerequisites

- Node.js 18+
- npm
- Wrangler CLI
- Cloudflare account
- Existing Cloudflare Worker project

---

## âš™ï¸ Setup

1. **Clone this repository**
   `git clone https://github.com/shiyoukh/cf_ai_agent.git && cd cf_ai_agent`

2. **Install dependencies**
   `npm install`

3. **Start server**
   `npm start`

---

## â˜ï¸ Configure Cloudflare AI

You can connect to Workers AI using **either** the native `[ai]` binding (recommended) or via API key + gateway URL.

### Option A â€” Native [ai] Binding (recommended)

In `wrangler.toml`:

```
[ai]
binding = "AI"
```

### Option B â€” Gateway / API Key (manual config)

If youâ€™re using a Cloudflare AI Gateway or external endpoint, set secrets:

```
wrangler secret put WORKERSAI_API_KEY
wrangler secret put GATEWAY_BASE_URL
```

These will be available inside your worker as `env.WORKERSAI_API_KEY` and `env.GATEWAY_BASE_URL`.

---

## ğŸ§  Running the Chatbot Locally

1. **Start the development server**
   `npm start`

2. **Open the local dev URL shown in your terminal** (usually `http://127.0.0.1:8787/`)
   Youâ€™ll see the **AI Chat Agent UI**. Try messages like:
   - `hello`
   - `what is 1+1?`
     If configured correctly, youâ€™ll get real Llama 3.3 responses streamed from Cloudflare Workers AI.

---

## ğŸš€ Deploy to Cloudflare

When ready to deploy:
`npm run deploy`
or directly:
`wrangler deploy`

---

## ğŸ§© Debugging

**Health Check:**
Visit `/check-open-ai-key` â€” returns `{ "success": true }` if your Worker is configured correctly.

**Debug Endpoint:**
Visit `/debug-model` â€” should return â€œokâ€ if the model is reachable.

---

## ğŸ›  Project Structure

```
src/
â”œâ”€â”€ index.ts              # Worker entrypoint (routes requests)
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ai-chat-agent.ts  # Base chat agent class
â”‚   â””â”€â”€ schedule.ts       # Schedule prompt logic
â”œâ”€â”€ tools/                # Tool definitions and execution handlers
â”œâ”€â”€ utils/                # Helper functions
```

---

## ğŸ’¡ Notes

- Default model: `@cf/meta/llama-3.3-70b-instruct-fp8-fast`
- Change model in `src/index.ts` by editing:
  `const MODEL_ID = "@cf/meta/llama-3.3-70b-instruct-fp8-fast";`
- Tool-calling is disabled by default for stability. To enable:
  `const SUPPORTS_TOOLS = true;`

---

## ğŸ§‘â€ğŸ’» Local Development Shortcuts

| Command          | Description            |
| ---------------- | ---------------------- |
| `npm start`      | Run worker locally     |
| `npm run deploy` | Deploy to Cloudflare   |
| `npm run build`  | Build TypeScript       |
| `wrangler tail`  | Stream production logs |

---

## ğŸ§¾ License

MIT License Â© 2025 â€” Built with â¤ï¸ by Ali Alshiyoukh

---

## âš¡ Quick Recap

1. Clone the repo
2. Install dependencies
3. Add `[ai]` binding or set secrets
4. Run `npm start`
5. Chat with your bot locally
6. Deploy using `wrangler deploy`

Enjoy your Llama-powered chatbot on Cloudflare ğŸ¦™â˜ï¸
