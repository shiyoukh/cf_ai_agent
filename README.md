# ğŸ§  AI Chat Agent (Cloudflare + Llama 3.3) 
# (LIVE ON [chatbot.shiyoukh.com](https://chatbot.shiyoukh.com))

An intelligent real-time chatbot powered by **Llama 3.3 (70B Instruct)** on **Cloudflare Workers AI**.  
This project runs a streaming chat agent with task scheduling and multi-session memory using Durable Objects, fully deployable on Cloudflare.

# This project is now live on [chatbot.shiyoukh.com](https://chatbot.shiyoukh.com)! I am currently self-hosting it.

---

## ğŸš€ Features

- âš¡ Powered by Llama 3.3 (70B-Instruct)
- ğŸ’¬ Real-time streaming chat responses
- ğŸ§  Persistent multi-session chat memory (via Durable Objects)
- ğŸ§© Extensible tool system for task automation
- ğŸ•’ Built-in schedule tool support
- â˜ï¸ Deployable on Cloudflare Workers
- ğŸ§± TypeScript-based, modular structure

---

## ğŸ§© Prerequisites

- Node.js 18+
- npm
- Wrangler CLI
- Cloudflare account
- Existing Cloudflare Worker project

---

## âš™ï¸ Setup

1. **Clone this repository**
   ```
   git clone https://github.com/shiyoukh/cf_ai_agent.git && cd cf_ai_agent
   ```

2. **Install dependencies**
   ```
   npm install
   ```

3. **Start the local server**
   ```
   npm start
   ```

---

## â˜ï¸ Configure Cloudflare AI

You can connect to Workers AI using **either** the native `[ai]` binding (recommended) or via API key + gateway URL.

### Option A â€” Native [ai] Binding (recommended)

In `wrangler.toml`:

```
[ai]
binding = "AI"
```

### Option B â€” Gateway / API Key (manual config)

If youâ€™re using a Cloudflare AI Gateway or external endpoint, set secrets:

```
wrangler secret put WORKERSAI_API_KEY
wrangler secret put GATEWAY_BASE_URL
```

These will be available inside your worker as `env.WORKERSAI_API_KEY` and `env.GATEWAY_BASE_URL`.

---

## ğŸ§  Running the Chatbot Locally

1. **Start the development server**
   ```
   npm start
   ```

2. **Open the local dev URL shown in your terminal**
   (usually `http://127.0.0.1:8787/`)

   Youâ€™ll see the **AI Chat Agent UI**. Try messages like:
   - `hello`
   - `what is 1+1?`

   If configured correctly, youâ€™ll get real Llama 3.3 responses streamed from Cloudflare Workers AI.

---

## ğŸ”¬ Testing the Components

This project includes a working front-end, Durable Object backend, and AI integration. You can test each module independently:

### âœ… 1. Chat Persistence (Durable Objects)

Each chat session is isolated using the `session` parameter in the URL:

- `http://localhost:5173/?session=a`
- `http://localhost:5173/?session=b`

Each session will maintain its own conversation history through the Durable Object state.

### âœ… 2. Follow-up Scheduling

Test the built-in scheduling feature:
1. Type a message in chat.
2. Click the clock (ğŸ•’) button.
3. Wait one minute â€” the scheduled follow-up will appear automatically from the server.
   This confirms that the Workerâ€™s `alarm()` event and storage are functioning.

### âœ… 3. History Retrieval

You can return to any of your chat sessions and your chats will be there (Auto deletion currently set at every 14 days).

### âœ… 4. Error Handling & Debug

Use the following endpoints for diagnostics:
- `/check-open-ai-key` â†’ returns `{ "success": true }` if Workers AI is configured.
- `/debug-model` â†’ should return â€œokâ€ if the model is reachable.

---

## ğŸš€ Deploy to Cloudflare

When ready to deploy:
```
npm run deploy
```
or directly:
```
wrangler deploy
```

---

## ğŸ›  Project Structure

```
src/
â”œâ”€â”€ server.ts            # Worker entrypoint (routes requests)
â”œâ”€â”€ chat-do.ts           # Durable Object logic (sessions, scheduling)
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ai-chat-agent.ts # Base chat agent class
â”‚   â””â”€â”€ schedule.ts      # Schedule prompt logic
â”œâ”€â”€ tools/               # Tool definitions and execution handlers
â”œâ”€â”€ utils/               # Helper functions
```

---

## ğŸ§  Architecture Overview

- **Frontend (Vite + React):**
  Manages chat UI, message streaming, scheduling button, and real-time updates.
- **Backend (Cloudflare Worker):**
  Routes chat, history, and scheduling requests.
- **Durable Object (`Chat`):**
  Maintains session memory and executes scheduled jobs.
- **Workers AI Integration:**
  Streams responses from Llama 3.3 running on Cloudflareâ€™s distributed inference engine.

---

## ğŸ’¡ Notes

- Default model: `@cf/meta/llama-3.3-70b-instruct-fp8-fast`
- Change model in `src/server.ts` by editing:
  ```
  const MODEL_ID = "@cf/meta/llama-3.3-70b-instruct-fp8-fast";
  ```
- Tool-calling is disabled by default for stability. To enable:
  ```
  const SUPPORTS_TOOLS = true;
  ```

---

## ğŸ§‘â€ğŸ’» Local Development Shortcuts

| Command          | Description            |
| ---------------- | ---------------------- |
| `npm start`      | Run worker locally     |
| `npm run deploy` | Deploy to Cloudflare   |
| `npm run build`  | Build TypeScript       |
| `wrangler tail`  | Stream production logs |

---

## âš¡ Troubleshooting

| Issue | Cause | Fix |
|-------|--------|-----|
| Bot replies â€œokâ€ only | AI inference failed (code 1031) | Check `/debug-model` and Workers AI config |
| History resets after clear | Page auto-refresh re-fetches history | Wait until next scheduled refresh cycle |
| â€œAI not configuredâ€ banner | Missing `[ai]` binding | Add `[ai] binding = "AI"` in wrangler.toml |

---

## ğŸ§¾ License

MIT License Â© 2025 â€” Built with â¤ï¸ by **Ali Alshiyoukh**

---

## âš¡ Quick Recap

1. Clone the repo  
2. Install dependencies  
3. Add `[ai]` binding or set secrets  
4. Run `npm start`  
5. Chat with your bot locally  
6. Test multi-session, scheduling, and history  
7. Deploy using `wrangler deploy`

Enjoy your Llama-powered chatbot on Cloudflare ğŸ¦™â˜ï¸
